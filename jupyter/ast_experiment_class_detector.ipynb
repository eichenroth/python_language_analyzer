{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./files/hierarchical_softmax.py', 'r') as f:\n",
    "    file_string = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import copy\n",
      "\n",
      "import numpy\n",
      "import six\n",
      "\n",
      "from chainer.backends import cuda\n",
      "from chainer import function\n",
      "from chainer.initializers import uniform\n",
      "from chainer import link\n",
      "from chainer.utils import type_check\n",
      "from chainer import variable\n",
      "\n",
      "\n",
      "class TreeParser(object):\n",
      "\n",
      "    def __init__(self):\n",
      "        self.next_id = 0\n",
      "\n",
      "    def size(self):\n",
      "        return self.next_id\n",
      "\n",
      "    def get_paths(self):\n",
      "        return self.paths\n",
      "\n",
      "    def get_codes(self):\n",
      "        return self.codes\n",
      "\n",
      "    def parse(self, tree):\n",
      "        self.next_id = 0\n",
      "        self.path = []\n",
      "        self.code = []\n",
      "        self.paths = {}\n",
      "        self.codes = {}\n",
      "        self._parse(tree)\n",
      "\n",
      "        assert(len(self.path) == 0)\n",
      "        assert(len(self.code) == 0)\n",
      "        assert(len(self.paths) == len(self.codes))\n",
      "\n",
      "    def _parse(self, node):\n",
      "        if isinstance(node, tuple):\n",
      "            # internal node\n",
      "            if len(node) != 2:\n",
      "                raise ValueError(\n",
      "                    'All internal nodes must have two child nodes')\n",
      "            left, right = node\n",
      "            self.path.append(self.next_id)\n",
      "            self.next_id += 1\n",
      "            self.code.append(1.0)\n",
      "            self._parse(left)\n",
      "\n",
      "            self.code[-1] = -1.0\n",
      "            self._parse(right)\n",
      "\n",
      "            self.path.pop()\n",
      "            self.code.pop()\n",
      "\n",
      "        else:\n",
      "            # leaf node\n",
      "            self.paths[node] = numpy.array(self.path, dtype=numpy.int32)\n",
      "            self.codes[node] = numpy.array(self.code, dtype=numpy.float32)\n",
      "\n",
      "\n",
      "class BinaryHierarchicalSoftmaxFunction(function.Function):\n",
      "\n",
      "    \"\"\"Hierarchical softmax function based on a binary tree.\n",
      "\n",
      "    This function object should be allocated beforehand, and be copied on every\n",
      "    forward computation, since the initializer parses the given tree. See the\n",
      "    implementation of :class:`BinaryHierarchicalSoftmax` for details.\n",
      "\n",
      "    Args:\n",
      "        tree: A binary tree made with tuples like ``((1, 2), 3)``.\n",
      "\n",
      "    .. seealso::\n",
      "       See :class:`BinaryHierarchicalSoftmax` for details.\n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, tree):\n",
      "        parser = TreeParser()\n",
      "        parser.parse(tree)\n",
      "        paths = parser.get_paths()\n",
      "        codes = parser.get_codes()\n",
      "        n_vocab = max(paths.keys()) + 1\n",
      "\n",
      "        self.paths = numpy.concatenate(\n",
      "            [paths[i] for i in range(n_vocab) if i in paths])\n",
      "        self.codes = numpy.concatenate(\n",
      "            [codes[i] for i in range(n_vocab) if i in codes])\n",
      "        begins = numpy.empty((n_vocab + 1,), dtype=numpy.int32)\n",
      "        begins[0] = 0\n",
      "        for i in range(0, n_vocab):\n",
      "            length = len(paths[i]) if i in paths else 0\n",
      "            begins[i + 1] = begins[i] + length\n",
      "        self.begins = begins\n",
      "\n",
      "        self.parser_size = parser.size()\n",
      "\n",
      "    def check_type_forward(self, in_types):\n",
      "        type_check.expect(in_types.size() == 3)\n",
      "        x_type, t_type, w_type = in_types\n",
      "\n",
      "        type_check.expect(\n",
      "            x_type.dtype == numpy.float32,\n",
      "            x_type.ndim == 2,\n",
      "            t_type.dtype == numpy.int32,\n",
      "            t_type.ndim == 1,\n",
      "            x_type.shape[0] == t_type.shape[0],\n",
      "            w_type.dtype == numpy.float32,\n",
      "            w_type.ndim == 2,\n",
      "            w_type.shape[0] == self.parser_size,\n",
      "            w_type.shape[1] == x_type.shape[1],\n",
      "        )\n",
      "\n",
      "    def to_gpu(self, device=None):\n",
      "        with cuda._get_device(device):\n",
      "            self.paths = cuda.to_gpu(self.paths)\n",
      "            self.codes = cuda.to_gpu(self.codes)\n",
      "            self.begins = cuda.to_gpu(self.begins)\n",
      "\n",
      "    def to_cpu(self):\n",
      "        self.paths = cuda.to_cpu(self.paths)\n",
      "        self.codes = cuda.to_cpu(self.codes)\n",
      "        self.begins = cuda.to_cpu(self.begins)\n",
      "\n",
      "    def forward_cpu(self, inputs):\n",
      "        x, t, W = inputs\n",
      "\n",
      "        loss = numpy.float32(0.0)\n",
      "        for ix, it in six.moves.zip(x, t):\n",
      "            loss += self._forward_cpu_one(ix, it, W)\n",
      "        return numpy.array(loss),\n",
      "\n",
      "    def _forward_cpu_one(self, x, t, W):\n",
      "        begin = self.begins[t]\n",
      "        end = self.begins[t + 1]\n",
      "\n",
      "        w = W[self.paths[begin:end]]\n",
      "        wxy = w.dot(x) * self.codes[begin:end]\n",
      "        loss = numpy.logaddexp(0.0, -wxy)  # == log(1 + exp(-wxy))\n",
      "        return numpy.sum(loss)\n",
      "\n",
      "    def backward_cpu(self, inputs, grad_outputs):\n",
      "        x, t, W = inputs\n",
      "        gloss, = grad_outputs\n",
      "        gx = numpy.empty_like(x)\n",
      "        gW = numpy.zeros_like(W)\n",
      "        for i, (ix, it) in enumerate(six.moves.zip(x, t)):\n",
      "            gx[i] = self._backward_cpu_one(ix, it, W, gloss, gW)\n",
      "        return gx, None, gW\n",
      "\n",
      "    def _backward_cpu_one(self, x, t, W, gloss, gW):\n",
      "        begin = self.begins[t]\n",
      "        end = self.begins[t + 1]\n",
      "\n",
      "        path = self.paths[begin:end]\n",
      "        w = W[path]\n",
      "        wxy = w.dot(x) * self.codes[begin:end]\n",
      "        g = -gloss * self.codes[begin:end] / (1.0 + numpy.exp(wxy))\n",
      "        gx = g.dot(w)\n",
      "        gw = g.reshape((g.shape[0], 1)).dot(x.reshape(1, x.shape[0]))\n",
      "        gW[path] += gw\n",
      "        return gx\n",
      "\n",
      "    def forward_gpu(self, inputs):\n",
      "        x, t, W = inputs\n",
      "        max_length = cuda.reduce(\n",
      "            'T t, raw T begins', 'T out', 'begins[t + 1] - begins[t]',\n",
      "            'max(a, b)', 'out = a', '0',\n",
      "            'binary_hierarchical_softmax_max_length')(t, self.begins)\n",
      "        max_length = cuda.to_cpu(max_length)[()]\n",
      "\n",
      "        length = max_length * x.shape[0]\n",
      "        ls = cuda.cupy.empty((length,), dtype=numpy.float32)\n",
      "        n_in = x.shape[1]\n",
      "        wxy = cuda.cupy.empty_like(ls)\n",
      "        cuda.elementwise(\n",
      "            '''raw T x, raw T w, raw int32 ts, raw int32 paths,\n",
      "            raw T codes, raw int32 begins, int32 c, int32 max_length''',\n",
      "            'T ls, T wxy',\n",
      "            '''\n",
      "            int ind = i / max_length;\n",
      "            int offset = i - ind * max_length;\n",
      "            int t = ts[ind];\n",
      "\n",
      "            int begin = begins[t];\n",
      "            int length = begins[t + 1] - begins[t];\n",
      "\n",
      "            if (offset < length) {\n",
      "              int p = begin + offset;\n",
      "              int node = paths[p];\n",
      "\n",
      "              T wx = 0;\n",
      "              for (int j = 0; j < c; ++j) {\n",
      "                int w_ind[] = {node, j};\n",
      "                int x_ind[] = {ind, j};\n",
      "                wx += w[w_ind] * x[x_ind];\n",
      "              }\n",
      "              wxy = wx * codes[p];\n",
      "              ls = log(1 + exp(-wxy));\n",
      "            } else {\n",
      "              ls = 0;\n",
      "            }\n",
      "            ''',\n",
      "            'binary_hierarchical_softmax_forward'\n",
      "        )(x, W, t, self.paths, self.codes, self.begins, n_in, max_length, ls,\n",
      "          wxy)\n",
      "        self.max_length = max_length\n",
      "        self.wxy = wxy\n",
      "        return ls.sum(),\n",
      "\n",
      "    def backward_gpu(self, inputs, grad_outputs):\n",
      "        x, t, W = inputs\n",
      "        gloss, = grad_outputs\n",
      "\n",
      "        n_in = x.shape[1]\n",
      "        gx = cuda.cupy.zeros_like(x)\n",
      "        gW = cuda.cupy.zeros_like(W)\n",
      "        cuda.elementwise(\n",
      "            '''T wxy, raw T x, raw T w, raw int32 ts, raw int32 paths,\n",
      "            raw T codes, raw int32 begins, raw T gloss,\n",
      "            int32 c, int32 max_length''',\n",
      "            'raw T gx, raw T gw',\n",
      "            '''\n",
      "            int ind = i / max_length;\n",
      "            int offset = i - ind * max_length;\n",
      "            int t = ts[ind];\n",
      "\n",
      "            int begin = begins[t];\n",
      "            int length = begins[t + 1] - begins[t];\n",
      "\n",
      "            if (offset < length) {\n",
      "              int p = begin + offset;\n",
      "              int node = paths[p];\n",
      "              T code = codes[p];\n",
      "\n",
      "              T g = -gloss[0] * code / (1.0 + exp(wxy));\n",
      "              for (int j = 0; j < c; ++j) {\n",
      "                int w_ind[] = {node, j};\n",
      "                int x_ind[] = {ind, j};\n",
      "                atomicAdd(&gx[x_ind], g * w[w_ind]);\n",
      "                atomicAdd(&gw[w_ind], g * x[x_ind]);\n",
      "              }\n",
      "            }\n",
      "            ''',\n",
      "            'binary_hierarchical_softmax_bwd'\n",
      "        )(self.wxy, x, W, t, self.paths, self.codes, self.begins, gloss, n_in,\n",
      "          self.max_length, gx, gW)\n",
      "        return gx, None, gW\n",
      "\n",
      "\n",
      "class BinaryHierarchicalSoftmax(link.Link):\n",
      "\n",
      "    \"\"\"Hierarchical softmax layer over binary tree.\n",
      "\n",
      "    In natural language applications, vocabulary size is too large to use\n",
      "    softmax loss.\n",
      "    Instead, the hierarchical softmax uses product of sigmoid functions.\n",
      "    It costs only :math:`O(\\\\log(n))` time where :math:`n` is the vocabulary\n",
      "    size in average.\n",
      "\n",
      "    At first a user need to prepare a binary tree whose each leaf is\n",
      "    corresponding to a word in a vocabulary.\n",
      "    When a word :math:`x` is given, exactly one path from the root of the tree\n",
      "    to the leaf of the word exists.\n",
      "    Let :math:`\\\\mbox{path}(x) = ((e_1, b_1), \\\\dots, (e_m, b_m))` be the path\n",
      "    of :math:`x`, where :math:`e_i` is an index of :math:`i`-th internal node,\n",
      "    and :math:`b_i \\\\in \\\\{-1, 1\\\\}` indicates direction to move at\n",
      "    :math:`i`-th internal node (-1 is left, and 1 is right).\n",
      "    Then, the probability of :math:`x` is given as below:\n",
      "\n",
      "    .. math::\n",
      "\n",
      "       P(x) &= \\\\prod_{(e_i, b_i) \\\\in \\\\mbox{path}(x)}P(b_i | e_i)  \\\\\\\\\n",
      "            &= \\\\prod_{(e_i, b_i) \\\\in \\\\mbox{path}(x)}\\\\sigma(b_i x^\\\\top\n",
      "               w_{e_i}),\n",
      "\n",
      "    where :math:`\\\\sigma(\\\\cdot)` is a sigmoid function, and :math:`w` is a\n",
      "    weight matrix.\n",
      "\n",
      "    This function costs :math:`O(\\\\log(n))` time as an average length of paths\n",
      "    is :math:`O(\\\\log(n))`, and :math:`O(n)` memory as the number of internal\n",
      "    nodes equals :math:`n - 1`.\n",
      "\n",
      "    Args:\n",
      "        in_size (int): Dimension of input vectors.\n",
      "        tree: A binary tree made with tuples like `((1, 2), 3)`.\n",
      "\n",
      "    Attributes:\n",
      "        W (~chainer.Variable): Weight parameter matrix.\n",
      "\n",
      "    See: Hierarchical Probabilistic Neural Network Language Model [Morin+,\n",
      "    AISTAT2005].\n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, in_size, tree):\n",
      "        # This function object is copied on every forward computation.\n",
      "        super(BinaryHierarchicalSoftmax, self).__init__()\n",
      "        self._func = BinaryHierarchicalSoftmaxFunction(tree)\n",
      "\n",
      "        with self.init_scope():\n",
      "            self.W = variable.Parameter(uniform.Uniform(1),\n",
      "                                        (self._func.parser_size, in_size))\n",
      "\n",
      "    def to_gpu(self, device=None):\n",
      "        with cuda._get_device(device):\n",
      "            super(BinaryHierarchicalSoftmax, self).to_gpu(device)\n",
      "            self._func.to_gpu(device)\n",
      "\n",
      "    def to_cpu(self):\n",
      "        super(BinaryHierarchicalSoftmax, self).to_cpu()\n",
      "        self._func.to_cpu()\n",
      "\n",
      "    @staticmethod\n",
      "    def create_huffman_tree(word_counts):\n",
      "        \"\"\"Makes a Huffman tree from a dictionary containing word counts.\n",
      "\n",
      "        This method creates a binary Huffman tree, that is required for\n",
      "        :class:`BinaryHierarchicalSoftmax`.\n",
      "        For example, ``{0: 8, 1: 5, 2: 6, 3: 4}`` is converted to\n",
      "        ``((3, 1), (2, 0))``.\n",
      "\n",
      "        Args:\n",
      "            word_counts (dict of int key and int or float values):\n",
      "                Dictionary representing counts of words.\n",
      "\n",
      "        Returns:\n",
      "            Binary Huffman tree with tuples and keys of ``word_coutns``.\n",
      "\n",
      "        \"\"\"\n",
      "        if len(word_counts) == 0:\n",
      "            raise ValueError('Empty vocabulary')\n",
      "\n",
      "        q = six.moves.queue.PriorityQueue()\n",
      "        # Add unique id to each entry so that we can compare two entries with\n",
      "        # same counts.\n",
      "        # Note that itreitems randomly order the entries.\n",
      "        for uid, (w, c) in enumerate(six.iteritems(word_counts)):\n",
      "            q.put((c, uid, w))\n",
      "\n",
      "        while q.qsize() >= 2:\n",
      "            (count1, id1, word1) = q.get()\n",
      "            (count2, id2, word2) = q.get()\n",
      "            count = count1 + count2\n",
      "            tree = (word1, word2)\n",
      "            q.put((count, min(id1, id2), tree))\n",
      "\n",
      "        return q.get()[2]\n",
      "\n",
      "    def __call__(self, x, t):\n",
      "        \"\"\"Computes the loss value for given input and ground truth labels.\n",
      "\n",
      "        Args:\n",
      "            x (~chainer.Variable): Input to the classifier at each node.\n",
      "            t (~chainer.Variable): Batch of ground truth labels.\n",
      "\n",
      "        Returns:\n",
      "            ~chainer.Variable: Loss value.\n",
      "\n",
      "        \"\"\"\n",
      "        f = copy.copy(self._func)  # creates a copy of the function node\n",
      "        return f(x, t, self.W)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(file_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<_ast.Import at 0x89dfef0>,\n",
       " <_ast.Import at 0x89dff50>,\n",
       " <_ast.Import at 0x89dffd0>,\n",
       " <_ast.ImportFrom at 0x89eb030>,\n",
       " <_ast.ImportFrom at 0x89eb070>,\n",
       " <_ast.ImportFrom at 0x89eb0b0>,\n",
       " <_ast.ImportFrom at 0x89eb0f0>,\n",
       " <_ast.ImportFrom at 0x89eb130>,\n",
       " <_ast.ImportFrom at 0x89eb170>,\n",
       " <_ast.ClassDef at 0x89eb1b0>,\n",
       " <_ast.ClassDef at 0x89dfcf0>,\n",
       " <_ast.ClassDef at 0x8a15e90>]"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_module = ast.parse(file_string)\n",
    "file_module.body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ClassVisitor(ast.NodeVisitor):\n",
    "    def __init__(self):\n",
    "        self._object_stack = []\n",
    "        self.detections = []\n",
    "\n",
    "    def generic_visit(self, node, detection=None):\n",
    "        if detection is None:\n",
    "            detection = Detection()\n",
    "\n",
    "        if hasattr(node, 'lineno'):\n",
    "            detection.begin = node.lineno\n",
    "\n",
    "        self._object_stack.append(detection)\n",
    "\n",
    "        super().generic_visit(node)\n",
    "\n",
    "        self._object_stack.pop()\n",
    "\n",
    "        if len(self._object_stack) > 0:\n",
    "            self._object_stack[-1].add_child(detection)\n",
    "        else:\n",
    "            self.detections.append(detection)\n",
    "\n",
    "    def visit_ClassDef(self, node):\n",
    "        detection = ClassDetection()\n",
    "        self.generic_visit(node, detection)\n",
    "\n",
    "    def visit_FunctionDef(self, node):\n",
    "        detection = FunctionDetection()\n",
    "        self.generic_visit(node, detection)\n",
    "\n",
    "    def visit_Module(self, node):\n",
    "        super().generic_visit(node)\n",
    "\n",
    "class Detection:\n",
    "    NAME = 'generic'\n",
    "\n",
    "    def __init__(self):\n",
    "        self.begin = None\n",
    "        self.end = None\n",
    "        self.children = []\n",
    "\n",
    "    def add_child(self, detected_child):\n",
    "        self.children.append(detected_child)\n",
    "\n",
    "\n",
    "class ClassDetection(Detection):\n",
    "    NAME = 'class'\n",
    "\n",
    "\n",
    "class FunctionDetection(Detection):\n",
    "    NAME = 'function'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "classVisitor = ClassVisitor()\n",
    "classVisitor.visit(file_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(classVisitor.detections[0].begin)\n",
    "print(classVisitor.detections[1].begin)\n",
    "print(classVisitor.detections[2].begin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.Detection at 0x8a1cf70>,\n",
       " <__main__.Detection at 0x8a1cf90>,\n",
       " <__main__.Detection at 0x8a1ce90>,\n",
       " <__main__.Detection at 0x8a1cd10>,\n",
       " <__main__.Detection at 0x8a1cc10>,\n",
       " <__main__.Detection at 0x8a1c810>,\n",
       " <__main__.Detection at 0x8a1cbb0>,\n",
       " <__main__.Detection at 0x8a1cc70>,\n",
       " <__main__.Detection at 0x8a1ca10>,\n",
       " <__main__.ClassDetection at 0x8a1cff0>,\n",
       " <__main__.ClassDetection at 0x8a1ccf0>,\n",
       " <__main__.ClassDetection at 0x8a1cb50>]"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classVisitor.detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.Detection at 0x8a1c950>,\n",
       " <__main__.FunctionDetection at 0x8a1cb70>,\n",
       " <__main__.FunctionDetection at 0x8a1ca30>,\n",
       " <__main__.FunctionDetection at 0x8a1c790>,\n",
       " <__main__.FunctionDetection at 0x8a1cb30>,\n",
       " <__main__.FunctionDetection at 0x8a1cdf0>,\n",
       " <__main__.FunctionDetection at 0x8a1cf50>]"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classVisitor.detections[9].children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
